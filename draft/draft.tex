\documentclass[times, 10pt, twocolumn]{article}
\usepackage{latex8}
\usepackage[dvips]{graphicx}
\usepackage[fleqn]{amsmath}
\usepackage{amsthm}
\usepackage{txfonts}
\usepackage{courier}
\usepackage{subfigure}
\usepackage{comment}
\usepackage{url}
\usepackage[square,numbers]{natbib}

%-----------------------------------------
% need for camera-ready
\pagestyle{empty}

%----------------------------------------

\begin{document}

\title{
Toward Adaptive GPU Resource Management for Embedded Real-Time Systems
}

\author {
Junsung Kim and Ragunathan (Raj) Rajkumar\\
\\
Department of Electrical and Computer Engineering\\
Carnegie Mellon University
\and
Shinpei Kato and Scott Brandt\\
\\
Department of Computer Science\\
University of California, Santa Cruz
}

\maketitle

%-----------------------------------------
% need for camera-ready
\thispagestyle{empty}

\begin{abstract}
 In this paper, we present conceptual frameworks for GPU applications to
 adjust their task execution times upon given workloads.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\section{System Model}
\label{sec:system_model}

We assume such embedded systems that equip GPUs as compute devices.
The application task starts execution on the CPU, and offloads its
data-parallel compute-intensive workload onto the GPU when needed.
Once offloaded onto the GPU, the task becomes non-preemptive due to many
reasons.
In fact, it is technically possible to preempt the running task on the
GPU by loading and restoring its context, but it requires additional
firmware, runtime, and OS support, and the preemption cost would be
non-trivial due to a very large set of GPU registers and states.
We hence restrict our attention to a non-preemptive execution model for
GPU computing.
GPUs may also pose some constraints in multi-tasking.
Even the NVIDIA Fermi architecture~\cite{Fermi}, one of the most popular
GPU product lines, allows only one context to use GPU resources at
once, though this context may spawn multiple GPU kernels (jobs)
simultaneously.
In other words, if task-level parallelism is required, the entire system
must run in the same context.
We however believe that this constraint will not limit the concept of
this paper.
We can use the same context to exploit concurrent parallel job
executions, and it should be enough to demonstrate a proof of concept. 
We also expect that future product lines will remove this concern.

We consider real-time applications where each task runs in a periodic or
sporadic manner under deadline constraints.
Such a task set and application includes motion planning and 
vision-based perception in state-of-the-art autonomous driving
vehicles, where the periods often correspond to frame-rates, and the
deadlines appear at the end of period.
It should however be noted that deadline constraints may not be relevant
to periods depending on applications, but the computation must meet the
deadline in order to achieve the desired quality of service in either case.
We also presume that the computing demand of each task is highly
variable.
For example, the performance of planning and perception is usually
governed by the number of objects, the size of data, and the desired
quality of output.
These workloads are also well parallelizable using the GPU.
Hence, the autonomous driving tasks and applications are our primary
target, but the contribution of this paper is not limited to those but
is also generally applicable to highly vaiable workloads running on the
GPU.

\section{Adaptive GPU Resource Management}
\label{sec:adaptivity_support}

In this section, we provide an idea of adaptivity support for GPU
applications.
We particularly focus on computing resource allocation problems.
It is true that some embedded real-time systems exhibit highly variable
workloads.
Since GPUs integrate a great number of compute cores on a chip, we can
support even highly variable workloads in a timely manner by adjusting
allocated cores at runtime.

There are several levels of approaches for adaptive GPU resource
management.
Previous work~\cite{Kato_RTAS11, Kato_RTSS11, Kato_ATC11} took
time-driven approaches that control timings and the amounts of time
allowed to access GPU resources, \textit{i.e.}, scheduling and
reservation.
In these time-driven approaches, application tasks need not to be aware
of what is happening in GPU resource management, because it is handled
by the OS or runtime scheduler.
However, they can never manage task execution times.
They also limit the number of contexts that can access the GPU
simultaneously to remove performance interference.
Therefore, GPU resources could be wasted if a running context does not
fully use compute cores.

We consider a different approach than previous work that enables GPU
applications to adjust their task execution times upon workload.
We believe that the number of cores used in the program is a major
factor that could change the execution time, and hence explore how to
adjust it at runtime.
It is important to note that programmer is typically responsible for
allocating the number of cores (or threads mapped to cores) in GPU
programming.
In order to adjust the number of cores at runtime, it is essential to
provide the programmer with an interface to obtain the information on
the number of cores available or allocated for the program at runtime.
The programmer is then responsible for making the program adaptive to
the number of cores.

In the following, we present two frameworks that could achieve our idea.
Although they are still at an idea stage, we plan to implement a real
system as a proof-of-concept, leveraging open-source
software~\cite{Kato_OSPERT11}.

\subsection{Explicit Adjustment}

We propose an explicit adjustment framework where the programmer is
responsible for adjusting the number of cores to relax or tighten
the computing demand.
There will be no adjustment unless the programmer explicitly takes an
action.
A typical usage of this framework with periodic real-time tasks is as
follows:

At the end of each period, the programmer calls a API function provided
by our framework that returns the latest task execution time.
The programmer next calls either of the following two API functions.
One increases the number of cores to be used in the next period or GPU
invocation to speed up the program.
The other decreases it to speed down the program.
This framework would work fine because the programmer often knows the
desired task execution time to meet the frame-rate or deadline.
It is also flexible in that the programmer can determine when to
increase or decrease the number of cores.

A downside of this framework is that some task may misbehave
and interfer with other contending application tasks, if the programmer
fails to call the API functions correctly.
We can cap the maximum number of cores available for an individual task
to prevent it from abusing GPU resources, but adaptivity of computing
depends on programmers, and the system cannot really control it.

\subsection{Implicit Adjustment}

Our second approach to adaptive resource management is an implicit
adjustment framework.
In this framework, the number of cores to be allocated for the program
is set by the runtime system.
Hence, adaptivity of computing does not really depend on programmers.
If the program is not aware of this framework, however, it may fail to
run, since the number of core allocated for the program may be different
from what the program assumes.

The programmer specifies the desired task execution time as a set point
before the task starts.
If this set point is not specified, the runtime system tries to derive
it internally as time goes by.
When the task uses the GPU, the runtime system consistently updates the
number of cores available for the corresponding task in the next period
based on the previous execution time records.
It is a programmer's duty to check the number of available cores before
offloading the computation onto the GPU.

This implicit adjustment framework is more encouraged than the explicit
adjustment framework, as it can enforce adaptive GPU resource
management.
However, it requires consensus in the programming model that the number
of cores allocated for the program could be changed every time it is
offloaded onto the GPU, and the programmer must be aware of it to make
the program work.
We claim that this is a natural trade-off between generality of
programming and adaptivity of service.

\subsection{Runtime System Support}
\label{sec:runtime}

The runtime system provides the API for programmers.
In order to support adaptive GPU resource management, we must provide
some additional API functions.

\begin{itemize}
 \item Our adaptive GPU resource management frameworks require a
       function to measure the execution time of each job running on the
       GPU.
       This function is easy-to-implement.
       Since we assume that job execution on the GPU is non-preemptive,
       the amount of time in run-to-completion can be accounted as a job
       execution time.
       This accounting method is known to work in previous
       work~\cite{Kato_ATC11, Rossbach_SOSP11}, and hence we can adopt
       it.
       For the explicit adjustment framework, this function must be
       exposed to the programmer, while it can also be used internally
       by the implicit adjustment framework.
 \item We also need several functions to change the number of cores to
       be allocated for the program.
       Some existing programming languages for GPGPU, \textit{e.g.},
       CUDA~\cite{CUDA}, provide the API to allow the programmer to
       specify the shape of grid structure and the number of threads
       mapped to compute cores.
       We can use this API as it is, or provide a corresponding API if
       the underlying programming language does not support it.
\end{itemize}

In addition to these API functions, the runtime system must be able to
detect when the program is offloaded onto the GPU and when it is
completed on the GPU.
Since the programmer calls a specific API function to launch the GPU
program in most GPU programming models, it is very easy to record the
start time of GPU execution.
Detection of the completion time of GPU execution, on the other hand, is
not straightforward.
We would need to use an interrupt to notify the runtime system of
completion of GPU execution.
Polling on a particular register is an alternative approach, but it
would not be suitable for latency-sensitive real-time systems, as
previous work demonstrated~\cite{Kato_ATC11}.

Finally, runtime system support must be integrated with the API so that
the programmer can make use of our frameworks under a single unified
programming model.
We plan to extend our CUDA runtime library developed in previous
work~\cite{Kato_OSPERT11} to support our adaptive frameworks.
This is our prototype implementation, and our frameworks can also be
integrated with other programming models beyond CUDA.

\section{Summary}
\label{sec:summary}

In this paper, we have discussed adaptivity requirements in embedded
real-time systems with GPUs, and presented two frameworks for adaptive
GPU resource management.
We conjecture that generality of programming may need to be compromised
to achieve adaptivity of resource allocation on the GPU.
Nonetheless, adaptive resource management is a key solution in
optimizing performance under resource-constrained environments.
We believe that our frameworks for GPU programming are useful
contributions in this line of work, given that trends are apparently
shifting to highly parallel heterogeneous computing.

\bibliographystyle{plain}
{\footnotesize
\bibliography{references}
}

\end{document}
