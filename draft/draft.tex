\documentclass[times, 10pt, twocolumn]{article}
\usepackage{latex8}
\usepackage[dvips]{graphicx}
\usepackage[fleqn]{amsmath}
\usepackage{amsthm}
\usepackage{txfonts}
\usepackage{courier}
\usepackage{subfigure}
\usepackage{comment}
\usepackage{url}
\usepackage[square,numbers]{natbib}

%-----------------------------------------
% need for camera-ready
\pagestyle{empty}

%----------------------------------------

\begin{document}

\title{
Toward Adaptive GPU Resource Management for Embedded Real-Time Systems
}

\author {
Junsung Kim and Ragunathan (Raj) Rajkumar\\
\\
Department of Electrical and Computer Engineering\\
Carnegie Mellon University
\and
Shinpei Kato and Scott Brandt\\
\\
Department of Computer Science\\
University of California, Santa Cruz
}

\maketitle

%-----------------------------------------
% need for camera-ready
\thispagestyle{empty}

\begin{abstract}
 In this paper, we present conceptual frameworks for GPU applications to
 adjust their task execution times upon given workloads. We propose two 
 frameworks enabling smart GPU resource management when many applications 
 share GPU resources while the workloads of those applications vary. 
 For {\it explicit adjustment}, we provide API functions so that application 
 developers can adjust the number of GPU cores depending on their needs. 
 An {\it implicit adjustment} will provide a run-time framwork which 
 dynamically allocates the number of cores to tasks based on the given 
 workloads. The runtime support of the proposed system can be realized using 
 functions which measure the execution times of the tasks on GPU and change 
 the number of GPU cores. We motivate the necessity of this framework 
 in the context of self-driving technologies, and we believe that our 
 frameworks for GPU programming are useful contributions given that 
 trends are apparently shifting to highly parallel heterogeneous computing.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Graphics processing units (GPUs) are widely used in a range of applications from supercomputers to mobile computers. For example, three of the top five supercomputers on the TOP500 list~\cite{TOP500} utilize GPUs to accelerate computations, and recent tablets such as ASUS Eee Pad Transformer Prime leverage 12 GPU cores of NVIDIAâ€™s well-known SoC, Tegra 3 to boost the performance while saving power~\cite{Tegra3}. This trend will only continue and in fact is expected to accelerate across multiple application domains. 

Modern automobiles have several tens of processing units, and advances in safe-driving features such as adaptive cruise control, stop-and-go cruise control, lane keeping and assisted lane change require even higher performance computing. As vehicles become semi-autonomous, drive autonomously on demand and eventually become fully autonomous, a multitude of computer vision, sensor fusion, signal processing and graphics problems must be solved in real time. Since solutions to those problems can leverage benefits of parallel processing techniques, GPUs can be used to accelerate computations. As an initial step, NVIDIA GPUs will be used for infotainment systems in future BMW vehicles~\cite{NVIDIA_BMW}.

Recent trends in self-driving cars~\cite{Urmson08, Markoff10, Kelly12} are motivated by various benefits of automated driving, but most importantly enabling automatic safety features. Those features are realized by the smart vehicle planning and the intelligent processing of data from various types of sensors on self-driving cars such as LIDAR (LIght Detection And Ranging), radar, camera, ultrasonic sensor, and so forth. One common characteristic of both the smart vehicle planning and the intelligent data processing is that their performances can be greatly accelerated using GPU, hence parallelism. For example, the more a self-driving car plans its potential paths to follow {\it a priori}, the better the quality of self-driving is. Since the self-driving car should follow only the best path, calculation of each potential path can happen parallel. A CMU team has already started using GPUs for autonomous driving computations. They have increased the performance of motion planning for autonomous driving up to 40 times using an NVIDIA GPU GTX 260 having 192 logical cores~\cite{McNaughton11}. 

The perception system which intelligently processes the data from various sensors can also obtain benefits using GPU. The perception system of a self-driving car should be able to detect, classify and track the obstacles around itself. Various types of sensors will generate huge amount of information to understand the surroundings. In order to process the data, a parallel processing technique is of vital importance. For example, a new self-driving car at CMU manages 1536 objects from LIDAR sensors before they are fused with other types of sensor data. There has been on-going research using GPU to build a perception system~\cite{Ferreira11}, and their GPU implementation yielded 30,000-times-faster performance compared to the case of using only CPU.

As described above, there has been research on applying GPU to different applications on self-driving cars, and it is clear that GPU can provide great benefits on realizing self-driving car technologies. However, not much research has been done when those technologies are deployed together on self-driving cars, where the loads of each application dynamically vary depending on environments. The period and the computation time of the planning algorithms for autonomous driving highly depend on the vehicle speed, so the planning algorithms can be heavily loaded when the car is on highway. The load of the perception algorithms mainly depend on the number of obstacles around the car hence the perception system requires more computing resources when the car is driving in the urban area. Therefore, an intelligent method of sharing many cores on GPU would be essential when we use GPU on self-driving cars. For example, if a self-driving car has a 96-core GPU, the planning algorithm of the car can use 72 cores on the highway and use 12 cores in the urban environment. A self-driving car~\cite{Urmson08} require tens of tasks, and the dynamic core management should be fulfilled across all tasks if those tasks utilize GPU. 

In this paper, we present conceptual frameworks for
GPU applications to adjust their task execution times upon
given workloads. We propose two frameworks enabling
smart GPU resource management when many applications
share GPU resources while the workloads of those appli-
cations vary. For explicit adjustment, we provide API func-
tions so that application developers can adjust the number
of GPU cores depending on their needs. An implicit adjust-
ment will provide a run-time framwork which dynamically
allocates the number of cores to tasks based on the given
workloads. The runtime support of the proposed system
can be realized using functions which measure the execu-
tion times of the tasks on GPU and change the number of
cores.


The rest of paper is organized as follows. Section~\ref{sec:system_model} describes how our proposed system is modeled. Section~\ref{sec:adaptivity_support} presents the methods for adaptively managing the given GPU resources, and we conclude our paper in Section~\ref{sec:summary}.

\section{System Model}
\label{sec:system_model}

We assume such embedded systems that equip GPUs as compute devices.
The application task starts execution on the CPU, and offloads its
data-parallel compute-intensive workload onto the GPU when needed.
Once offloaded onto the GPU, the task becomes non-preemptive due to many
reasons.
In fact, it is technically possible to preempt the running task on the
GPU by loading and restoring its context, but it requires additional
firmware, runtime, and OS support, and the preemption cost would be
non-trivial due to a very large set of GPU registers and states.
We hence restrict our attention to a non-preemptive execution model for
GPU computing.
GPUs may also pose some constraints in multi-tasking.
Even the NVIDIA Fermi architecture~\cite{Fermi}, one of the most popular
GPU product lines, allows only one context to use GPU resources at
once, though this context may spawn multiple GPU kernels (jobs)
simultaneously.
In other words, if task-level parallelism is required, the entire system
must run in the same context.
We however believe that this constraint will not limit the concept of
this paper.
We can use the same context to exploit concurrent parallel job
executions, and it should be enough to demonstrate a proof of concept. 
We also expect that future product lines will remove this concern.

We consider real-time applications where each task runs in a periodic or
sporadic manner under deadline constraints.
Such a task set and application includes motion planning and 
vision-based perception in state-of-the-art autonomous driving
vehicles, where the periods often correspond to frame-rates, and the
deadlines appear at the end of period.
It should however be noted that deadline constraints may not be relevant
to periods depending on applications, but the computation must meet the
deadline in order to achieve the desired quality of service in either case.
We also presume that the computing demand of each task is highly
variable.
For example, the performance of planning and perception is usually
governed by the number of objects, the size of data, and the desired
quality of output.
These workloads are also well parallelizable using the GPU.
Hence, the autonomous driving tasks and applications are our primary
target, but the contribution of this paper is not limited to those but
is also generally applicable to highly vaiable workloads running on the
GPU.

\section{Adaptive GPU Resource Management}
\label{sec:adaptivity_support}

In this section, we provide an idea of adaptivity support for GPU
applications.
We particularly focus on computing resource allocation problems.
It is true that some embedded real-time systems exhibit highly variable
workloads.
Since GPUs integrate a great number of compute cores on a chip, we can
support even highly variable workloads in a timely manner by adjusting
allocated cores at runtime.

There are several levels of approaches for adaptive GPU resource
management.
Previous work~\cite{Kato_RTAS11, Kato_RTSS11, Kato_ATC11} took
time-driven approaches that control timings and the amounts of time
allowed to access GPU resources, \textit{i.e.}, scheduling and
reservation.
In these time-driven approaches, application tasks need not to be aware
of what is happening in GPU resource management, because it is handled
by the OS or runtime scheduler.
However, they can never manage task execution times.
They also limit the number of contexts that can access the GPU
simultaneously to remove performance interference.
Therefore, GPU resources could be wasted if a running context does not
fully use compute cores.

We consider a different approach than previous work that enables GPU
applications to adjust their task execution times upon workload.
We believe that the number of cores used in the program is a major
factor that could change the execution time, and hence explore how to
adjust it at runtime.
It is important to note that programmer is typically responsible for
allocating the number of cores (or threads mapped to cores) in GPU
programming.
In order to adjust the number of cores at runtime, it is essential to
provide the programmer with an interface to obtain the information on
the number of cores available or allocated for the program at runtime.
The programmer is then responsible for making the program adaptive to
the number of cores.

In the following, we present two frameworks that could achieve our idea.
Although they are still at an idea stage, we plan to implement a real
system as a proof-of-concept, leveraging open-source
software~\cite{Kato_OSPERT11}.

\subsection{Explicit Adjustment}

We propose an explicit adjustment framework where the programmer is
responsible for adjusting the number of cores to relax or tighten
the computing demand.
There will be no adjustment unless the programmer explicitly takes an
action.
A typical usage of this framework with periodic real-time tasks is as
follows:

At the end of each period, the programmer calls a API function provided
by our framework that returns the latest task execution time.
The programmer next calls either of the following two API functions.
One increases the number of cores to be used in the next period or GPU
invocation to speed up the program.
The other decreases it to speed down the program.
This framework would work fine because the programmer often knows the
desired task execution time to meet the frame-rate or deadline.
It is also flexible in that the programmer can determine when to
increase or decrease the number of cores.

A downside of this framework is that some task may misbehave
and interfer with other contending application tasks, if the programmer
fails to call the API functions correctly.
We can cap the maximum number of cores available for an individual task
to prevent it from abusing GPU resources, but adaptivity of computing
depends on programmers, and the system cannot really control it.

\subsection{Implicit Adjustment}

Our second approach to adaptive resource management is an implicit
adjustment framework.
In this framework, the number of cores to be allocated for the program
is set by the runtime system.
Hence, adaptivity of computing does not really depend on programmers.
If the program is not aware of this framework, however, it may fail to
run, since the number of core allocated for the program may be different
from what the program assumes.

The programmer specifies the desired task execution time as a set point
before the task starts.
If this set point is not specified, the runtime system tries to derive
it internally as time goes by.
When the task uses the GPU, the runtime system consistently updates the
number of cores available for the corresponding task in the next period
based on the previous execution time records.
It is a programmer's duty to check the number of available cores before
offloading the computation onto the GPU.

This implicit adjustment framework is more encouraged than the explicit
adjustment framework, as it can enforce adaptive GPU resource
management.
However, it requires consensus in the programming model that the number
of cores allocated for the program could be changed every time it is
offloaded onto the GPU, and the programmer must be aware of it to make
the program work.
We claim that this is a natural trade-off between generality of
programming and adaptivity of service.

\subsection{Runtime System Support}
\label{sec:runtime}

The runtime system provides the API for programmers.
In order to support adaptive GPU resource management, we must provide
some additional API functions.

\begin{itemize}
 \item Our adaptive GPU resource management frameworks require a
       function to measure the execution time of each job running on the
       GPU.
       This function is easy-to-implement.
       Since we assume that job execution on the GPU is non-preemptive,
       the amount of time in run-to-completion can be accounted as a job
       execution time.
       This accounting method is known to work in previous
       work~\cite{Kato_ATC11, Rossbach_SOSP11}, and hence we can adopt
       it.
       For the explicit adjustment framework, this function must be
       exposed to the programmer, while it can also be used internally
       by the implicit adjustment framework.
 \item We also need several functions to change the number of cores to
       be allocated for the program.
       Some existing programming languages for GPGPU, \textit{e.g.},
       CUDA~\cite{CUDA}, provide the API to allow the programmer to
       specify the shape of grid structure and the number of threads
       mapped to compute cores.
       We can use this API as it is, or provide a corresponding API if
       the underlying programming language does not support it.
\end{itemize}

In addition to these API functions, the runtime system must be able to
detect when the program is offloaded onto the GPU and when it is
completed on the GPU.
Since the programmer calls a specific API function to launch the GPU
program in most GPU programming models, it is very easy to record the
start time of GPU execution.
Detection of the completion time of GPU execution, on the other hand, is
not straightforward.
We would need to use an interrupt to notify the runtime system of
completion of GPU execution.
Polling on a particular register is an alternative approach, but it
would not be suitable for latency-sensitive real-time systems, as
previous work demonstrated~\cite{Kato_ATC11}.

Finally, runtime system support must be integrated with the API so that
the programmer can make use of our frameworks under a single unified
programming model.
We plan to extend our CUDA runtime library developed in previous
work~\cite{Kato_OSPERT11} to support our adaptive frameworks.
This is our prototype implementation, and our frameworks can also be
integrated with other programming models beyond CUDA.

\section{Summary}
\label{sec:summary}

In this paper, we have discussed adaptivity requirements in embedded
real-time systems with GPUs, and presented two frameworks for adaptive
GPU resource management.
We conjecture that generality of programming may need to be compromised
to achieve adaptivity of resource allocation on the GPU.
Nonetheless, adaptive resource management is a key solution in
optimizing performance under resource-constrained environments.
We believe that our frameworks for GPU programming are useful
contributions in this line of work, given that trends are apparently
shifting to highly parallel heterogeneous computing.

\bibliographystyle{plain}
{\footnotesize
\bibliography{references}
}

\end{document}
